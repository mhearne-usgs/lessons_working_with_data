{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libcomcat.search import get_event_by_id\n",
    "from libcomcat.dataframes import get_pager_data_frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import time\n",
    "from scipy.optimize import curve_fit\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with tabular data in Python generally means working with the [pandas](https://pandas.pydata.org/) library. For the purposes of this discussion, pandas provides two useful data structures: A Series object (analogous to a dictionary), and a DataFrame object, which is analogous to an Excel spreadsheet, from which columns or rows can be extracted as Series objects.\n",
    "\n",
    "DataFrames can be read in from delimited (comma, tab, etc.) text files, Excel files, database tables, or created from scratch. DataFrame objects provide a number of useful methods for data exploration and analysis including:\n",
    "\n",
    " - computation\n",
    " - statistics\n",
    " - plotting\n",
    " \n",
    "In this notebook, we'll be reading, \"cleaning\", and exploring one data set.\n",
    " \n",
    "## Contents\n",
    "----\n",
    "- [Reading Delimited Data](#Reading-Delimited-Data)\n",
    "- [Cleaning Data](#Cleaning-Data)\n",
    "  - [Indexing and Selecting](#Indexing-and-Selecting)\n",
    "  - [Fixing Columns](#Fixing-Columns)\n",
    "  - [Extracting Data From Columns](#Extracting-Data-From-Columns)\n",
    "  - [Subsetting Data](#Subsetting-Data)\n",
    "  - [Removing Duplicates](#Removing-Duplicates)\n",
    "  - [Merging DataFrames](#Merging-DataFrames)\n",
    "- [Exploring Data](#Exploring-Data)\n",
    "  - [Comparing Observed and Predicted](#Comparing-Observed-and-Predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Delimited Data\n",
    "\n",
    "pandas can read data from a number of formats:\n",
    "\n",
    " - Comma* separated values (CSV)\n",
    " - Excel spreadsheets\n",
    " - Hierarchical Data Format (HDF) file\n",
    " - Relational databases\n",
    " \n",
    "\\*For reading CSV files, pandas provides a function called *read_csv()*. Despite its name, this function is not limited to reading text files with columns separated by commas. The delimiter can be just about anything - tabs are very common, as are pipes (\"|\"), as we see below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set was extracted from the Hydra QA database, and represents USGS National Earthquake Information Center (NEIC) analyst reports collected on earthquake impacts. There are a few things to note about this file that we will have to deal with before doing any analysis with it. This is known as \"data cleaning\".\n",
    "\n",
    " - As noted above the delimiter is a \"|\" character.\n",
    " - There are spaces around the column names\n",
    " - The last three lines of the file do not match the format in the rest of the file\n",
    " - Most of the data of interest is contained in the \"spassportentry\" column, and consists of columns separated by whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've written two functions below that allow us to quickly see the beginning and ending lines of a file, similar to the *head* and *tail* commands available on OSX and Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(filename, nlines):\n",
    "    with open(filename, 'rt') as f:\n",
    "        for i in range(0,nlines):\n",
    "            print('*'+f.readline()+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail(filename, nlines):\n",
    "    lines = open(filename, 'rt').readlines()\n",
    "    for i in range(-nlines,-1):\n",
    "        print('*'+lines[i]+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_file = '../data/hydra_impact_comments.txt'\n",
    "head(impact_file,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(impact_file,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the documentation for read_csv [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), or by executing \"pd.read_csv?\" in a code cell.\n",
    "\n",
    "Note that there are several options we can use to help ourselves out right off the bat:\n",
    "\n",
    " - the *parse_dates* option will allow us to read in the \"ot\" (origin time) column as a pandas Timestamp *object*, not just a string.\n",
    " - While we could read in the columns from the file and then fix the extra spaces, it's easier to use the *skiprows* option to skip the first line and provide our own column names.\n",
    " - We can use the *skipfooter* option to ignore those final three problematic lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hydra_id', 'hydra_time', 'hydra_magnitude', 'hydra_command', 'passport_entry']\n",
    "hydra_impacts = pd.read_csv(impact_file, names=columns, engine='python',\n",
    "                            parse_dates=['hydra_time'], skiprows=1, \n",
    "                            skipfooter=3, delimiter='|')\n",
    "hydra_impacts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that our timestamp parsing worked, by asking a question about *duration* of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_impacts['hydra_time'].max() - hydra_impacts['hydra_time'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Columns\n",
    "Next, let's convert that hydra ID to a ComCat-compatible event ID by prepending 'us' to each ID, and lowercasing the column while we're at it. Fortunately, pandas provides a number of string methods that can operate on all elements of an array (column, row, etc.) These are well-described [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#string-methods). Many of these functions should be familiar to anyone who has used the methods of the standard Python *str* object. \n",
    "\n",
    "It turns out that some of these hydra_id values have extraneous spaces in them as well. You can chain multiple string operations together, as seen below with lower() and strip(). \n",
    "\n",
    "Finally, it turns out that there are [multiple ways](https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas) to prepend a string to the values in a column. Once again StackOverflow comes to the rescue. This was the most syntactically straighforward solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_impacts['hydra_id'] = hydra_impacts['hydra_id'].str.lower().str.strip()\n",
    "hydra_impacts['hydra_command'] = hydra_impacts['hydra_command'].str.strip()\n",
    "hydra_impacts['hydra_id'] = 'us' + hydra_impacts['hydra_id'].astype(str)\n",
    "hydra_impacts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, seeing magnitude reported beyond 1 significant digit is not useful. Let's round those values to the nearest tenth of a magnitude unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra_impacts['hydra_magnitude'] = hydra_impacts['hydra_magnitude'].round(decimals=1)\n",
    "hydra_impacts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Selecting\n",
    "\n",
    "You may be wondering at this point what the unlabeled column at the left of our DataFrame is for. This is an *index* column, and the values in it are used as labels for the rows. By default, pandas assigns integer labels for the index, but this index could be any kind of variable. In our situation, the most logical candidate for an index is the hydra_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_impacts = hydra_impacts.set_index('hydra_id')\n",
    "tmp_impacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting single event by label (.loc) # this returns a series\n",
    "tmp_impacts.loc['usc0004ktb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting multiple events by label (.loc)\n",
    "tmp_impacts.loc[['usc0004ktb','usc0005idz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a column from multiple events \n",
    "tmp_impacts.loc[['usc0004ktb','usc0005idz'],'passport_entry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting multiple events by position\n",
    "tmp_impacts.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting single row by position\n",
    "tmp_impacts.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting by row and column positions\n",
    "tmp_impacts.iloc[0:2,[1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common use cases in our experience, however, is sub-selecting a DataFrame based on criteria in one of more colums. \n",
    "\n",
    "If you recall from numpy, you select elements of an array most efficiently by doing:\n",
    "\n",
    "```x = np.arange(1,6)```\n",
    "\n",
    "returns: array([1, 2, 3, 4, 5])\n",
    "\n",
    "```x > 3```\n",
    "\n",
    "returns: array([False, False, False,  True,  True], dtype=bool)\n",
    "\n",
    "\n",
    "```x[x > 3]``` \n",
    "\n",
    "returns: array([4, 5])\n",
    "\n",
    "\n",
    "The same sort of selection syntax is possible in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_impacts[tmp_impacts['hydra_time'] > pd.Timestamp('2020-06-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make complex selections - for example, we might want to see all events in 2019 that are greater than M6.0. If you have multiple selection criteria, you can split those criteria into different arrays and combine them with boolean operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_2019 = tmp_impacts['hydra_time'] > pd.Timestamp('2019-01-01')\n",
    "end_2019 = tmp_impacts['hydra_time'] > pd.Timestamp('2019-12-31 23:59:59')\n",
    "mag_6plus = tmp_impacts['hydra_magnitude'] > 6.0\n",
    "tmp_impacts[start_2019 & end_2019 & mag_6plus].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data From Columns\n",
    "Now for the hard part - we need to parse the impact data out of the passport_entry column. To handle this, we'll need to iterate over the rows of the dataframe, pull apart the passport_entry column, and create a new DataFrame. You can construct new DataFrame objects in a number of different ways - we'll create a list of Series objects and pass that to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPACT_COLUMNS = ['LossExtent',\n",
    "                  'EffectType',\n",
    "                  'LossQuantifier',\n",
    "                  'LossValue',\n",
    "                  'Location',\n",
    "                  'CollectionSource',\n",
    "                  'database_id',\n",
    "                  'comment']\n",
    "rows = []\n",
    "for idx, hydra_row in hydra_impacts.iterrows():\n",
    "    passport_parts = hydra_row['passport_entry'].split()\n",
    "    # make a dictionary out of the passport entry\n",
    "    pdict = dict(zip(IMPACT_COLUMNS, passport_parts))\n",
    "    new_row_dict = hydra_row.to_dict()\n",
    "    new_row_dict.update(pdict)\n",
    "    rows.append(new_row_dict)\n",
    "    \n",
    "impacts = pd.DataFrame(rows)\n",
    "# we need to change the data type of the LossValue column to an integer\n",
    "impacts['LossValue'] = impacts['LossValue'].astype(np.int32)\n",
    "impacts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the passport_entry column anymore, so we use the *drop* method to get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts.drop('passport_entry', axis='columns', inplace=True)\n",
    "impacts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data\n",
    "Because we're only interested in fatalities due to shaking for the purpose of this analysis, let's extract just those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = impacts['LossExtent'] == 'Deaths'\n",
    "condition2 = impacts['EffectType'] == 'Shaking'\n",
    "condition3 = impacts['hydra_command'] == 'PubFlagsAddImpact'\n",
    "deaths = impacts[condition1 & condition2 & condition3]\n",
    "deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "You may notice that we have multiple fatality entries for some events. If this were a *true* analysis, we would do something clever and appropriate to determine the correct number of fatalities. Here we'll just choose the maximum value for each event, using an answer found from [StackOverflow](https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-count-in-groups-using-groupby)*. We'll also re-sort the data in time ascending order once we're finishing removing duplicates.\n",
    "\n",
    "*Google and stack overflow are going to be your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = deaths.groupby(['hydra_id'])['LossValue'].transform(max) == deaths['LossValue']\n",
    "#newdeaths = deaths[idx]\n",
    "newdeaths = deaths.sort_values('LossValue', ascending=False).drop_duplicates(['hydra_id'])\n",
    "newdeaths.sort_values('hydra_time', inplace=True)\n",
    "newdeaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wanted to see the distribution of magnitudes for the dataset as a bar chart. We could extract the number of occurrences of unique values using the `value_counts` method. Here we apply `value_counts` to the hydra_magnitude column. This returns a series with the first column being the unique value and the second column showing the number of occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "impact_data = hydra_impacts.drop_duplicates(subset='hydra_id')\n",
    "magnitudes = impact_data['hydra_magnitude'].value_counts()\n",
    "magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each unique magnitude has an associated number of occurrences; if we plot this as a bar chart, it is going to be very hard to decipher the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "ax = magnitudes.plot.bar()\n",
    "ystr = ax.set_ylabel('Number of occurrences', fontsize=18)\n",
    "xstr = ax.set_xlabel('Magnitude', fontsize=18)\n",
    "ticks = ax.set_xticklabels(magnitudes.index, rotation=60, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be much easier to evaluate this data if it were in bins. We can redo the value count with a set number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = impact_data['hydra_magnitude'].value_counts(bins=10)\n",
    "magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that the bins are not sorted. They are instead in order of occurrence. We can sort this before plotting using the sort method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_magnitudes = magnitudes.sort_index()\n",
    "sorted_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "ax = sorted_magnitudes.plot.bar()\n",
    "ystr = ax.set_ylabel('Number of occurrences', fontsize=18)\n",
    "xstr = ax.set_xlabel('Magnitude', fontsize=18)\n",
    "xticks = ax.set_xticklabels(sorted_magnitudes.index,\n",
    "                           rotation=60, fontsize=14) \n",
    "yticks = ax.set_yticklabels(np.arange(0, 140, 20), fontsize=14) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames\n",
    "\n",
    "Let's say we want to compare these *observed* fatalities with the *predicted* fatalities from PAGER. We can extract those fatalities from ComCat using the libcomcat tools. It takes about 90 seconds to download this data, so we'll load it from a pre-fetched file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expframe = pd.read_csv('../data/pager_exposures_hydra.csv')\n",
    "expframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the cell below, note that you'll be overriding the DataFrame *expframe* we read in above. Even if you don't run this cell, you should examine it carefully to notice some new things:\n",
    "\n",
    " - We are iterating over rows using the DataFrame *iterrows()* method, which returns the index and a reference to the Series representing the row of data.\n",
    " - We are also using two functions from the libcomcat library:\n",
    "   - *get_event_by_id()* - This returns a DetailEvent object, from which you can extract *authoritative* event information.\n",
    "   - *get_pager_data_frame()* This returns (in this case) a DataFrame with one row of PAGER exposure/loss information, or None if no PAGER results were created for the event.\n",
    " - We're also demonstrating the use of the DataFrame *to_csv()* method. Saving DataFrames to CSV or Excel is pretty straightforward, so we'll just point users to the documentation for [to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) and [to_excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel)\n",
    " \n",
    "#### To re-create the *expframe* variable:\n",
    "\n",
    " - Change the type of the cell below from \"Markdown\" to \"Code\"\n",
    " - Remove the leading/trailing triple backticks \\`\\`\\`\n",
    " - Execute the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# ---------------------------------------------------------\n",
    "exprows = []\n",
    "t1 = time.time()\n",
    "for idx, row in newdeaths.iterrows():\n",
    "    detail = get_event_by_id(row['hydra_id'])\n",
    "    expdict = {}\n",
    "    # fill in columns with ComCat authoritative values\n",
    "    expdict['hydra_id'] = row['hydra_id']\n",
    "    expdict['eventid'] = detail.id\n",
    "    expdict['time'] = pd.to_datetime(detail.time)\n",
    "    expdict['latitude'] = detail.latitude\n",
    "    expdict['longitude'] = detail.longitude\n",
    "    expdict['depth'] = detail.depth\n",
    "    expdict['magnitude'] = detail.magnitude\n",
    "    \n",
    "    # now retrieve the PAGER exposure data for this event\n",
    "    expframe = get_pager_data_frame(detail, get_losses=True)\n",
    "    if expframe is None:\n",
    "        msg = (f\"event {row['hydra_id']} \"\n",
    "               f\"Date: {row['hydra_time']} \"\n",
    "               f\"Magnitude: {row['hydra_magnitude']}\")\n",
    "        print(f'No PAGER information found for {msg}. Skipping.')\n",
    "        continue\n",
    "    exprow = expframe.iloc[0]\n",
    "    expdict['predicted_deaths'] = exprow['predicted_fatalities']\n",
    "    expdict['mmi5'] = exprow['mmi5']\n",
    "    expdict['mmi6'] = exprow['mmi6']\n",
    "    expdict['mmi7'] = exprow['mmi7']\n",
    "    expdict['mmi8'] = exprow['mmi8']\n",
    "    expdict['mmi9'] = exprow['mmi9']\n",
    "    expdict['mmi10'] = exprow['mmi10']\n",
    "    exprows.append(expdict)\n",
    "expframe = pd.DataFrame(exprows)\n",
    "t2 = time.time()\n",
    "print(f'Elapsed time: {t2-t1:.1f} seconds')\n",
    "expframe.head()\n",
    "expframe.to_csv('../data/pager_exposures_hydra.csv', index=False)\n",
    "# ---------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to merge the *observed* fatality dataframe we've created with the *predicted* fatality/exposure dataset we've extracted from ComCat. pandas DataFrame objects provide us with the *merge* method, which is described in detail [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expdeaths = newdeaths.merge(expframe, on='hydra_id')\n",
    "expdeaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Observed and Predicted\n",
    "\n",
    "We can now inspect the resulting DataFrame and see that we have all of the columns from both input DataFrames. At this point, the data has been effectively cleaned, and we can begin doing some simple data exploration and analysis.\n",
    "\n",
    "The PAGER project calculates estimated fatalities for an event based on a ShakeMap input and country-specific loss models. When we want to get a visual sense of how the models are performing compared to observations of fatalities, it is useful to compare them within the ranges defined for PAGER alert levels, which are:\n",
    "\n",
    " - Green: 0 fatalities (no response needed)\n",
    " - Yellow: 1-99 fatalities (local/regional response)\n",
    " - Orange: 100-999 fatalities (national response)\n",
    " - Red: 1000+ fatalities (international response)\n",
    " \n",
    "Our assumption is, for example, that the ground *response* to an earthquake with 25 fatalities will be about the same as one with 50 fatalities. So, we make a figure that demonstrates how often our predictions fall within the observed range, as with the plot below.\n",
    "\n",
    "#### Sidebar: \"A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "\n",
    "You may see this excessively passive voice error coming from pandas at some point, and wonder what the heck it means. \n",
    "\n",
    "When using pandas indexing to select a row or column from a DataFrame, what you get back is a *reference* to the same data that is in the original DataFrame. If you then try to *modify* this extracted row or column, you'll get the above error, along with a link to [this page](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy).\n",
    "\n",
    "A well-written page describing what's going on here, and how to solve it:\n",
    "\n",
    "[https://www.dataquest.io/blog/settingwithcopywarning/](https://www.dataquest.io/blog/settingwithcopywarning/)\n",
    "\n",
    "We bring this up here because of the following lines of code in the cell below:\n",
    "\n",
    "```\n",
    "observed = expdeaths['LossValue'].to_numpy()\n",
    "predicted = expdeaths['predicted_deaths'].to_numpy()\n",
    "observed[observed == 0] = 0.01\n",
    "predicted[predicted == 0] = 0.01\n",
    "```\n",
    "\n",
    "Adding the *to_numpy()* method to convert the observed and predicted fatality columns to numpy arrays explicitly makes copies of those two columns, thus avoiding the warning and doing (in this case) what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.75\n",
    "GREEN = (0/255,176/255,79/255,ALPHA)\n",
    "YELLOW = (255/255,255/255,0/255,ALPHA)\n",
    "ORANGE = (255/255,153/255,0/255,ALPHA)\n",
    "RED = (255/255,0/255,0/255,ALPHA)\n",
    "fig = plt.figure(figsize=(10,10));\n",
    "plt.loglog([0.001,1e7], [0.001,1e7],'w.');\n",
    "ax = fig.axes[0]\n",
    "errorboxes = []\n",
    "rgreen = Rectangle((0.001,0.001),1,1, facecolor=GREEN, edgecolor='black');\n",
    "ryellow = Rectangle((1,1),100,100, facecolor=YELLOW, edgecolor='black');\n",
    "rorange = Rectangle((100,100),1000,1000, facecolor=ORANGE, edgecolor='black');\n",
    "rred = Rectangle((1000,1000),1e7,1e7, facecolor=RED, edgecolor='black');\n",
    "ax.add_patch(rgreen);\n",
    "ax.add_patch(ryellow);\n",
    "ax.add_patch(rorange);\n",
    "ax.add_patch(rred);\n",
    "observed = expdeaths['LossValue'].to_numpy()\n",
    "predicted = expdeaths['predicted_deaths'].to_numpy()\n",
    "observed[observed == 0] = 0.01\n",
    "predicted[predicted == 0] = 0.01\n",
    "plt.loglog(observed, predicted, 'b.');\n",
    "xlocs, _ = plt.xticks();\n",
    "ylocs, _ = plt.yticks();\n",
    "xlocs = [0.001, 1, 10, 100, 1000, 1e5, 1e7]\n",
    "ylocs = [0.001, 1, 10, 100, 1000, 1e5, 1e7]\n",
    "newlabels = ['0', '1', '10', '$10^2$', '$10^3$', '$10^5$', '$10^7$']\n",
    "plt.xticks(xlocs, newlabels);\n",
    "plt.yticks(ylocs, newlabels);\n",
    "plt.axis([0.001,1e7, 0.001, 1e7]);\n",
    "plt.title('Observed Fatalities vs PAGER Predicted');\n",
    "plt.text(0.01, 1e6,f'N = {len(predicted)}', fontsize=16);\n",
    "plt.xlabel('Observed Fatalities');\n",
    "plt.ylabel('Predicted Fatalities');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB: Note here that our sample size is only 51 events, so take this \"analysis\" with a grain of salt.*\n",
    "\n",
    "You could possibly infer from the above plot that the PAGER system may:\n",
    " - underpredict events that have 0 observed fatalities\n",
    " - do a reasonable job at predicting \"Yellow\" alert events\n",
    " - overpredict events in the \"Orange\" alert range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
